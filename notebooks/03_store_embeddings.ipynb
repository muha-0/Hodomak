{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bff8420",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891dc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import SiglipProcessor, SiglipModel\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from qdrant_client.models import Batch\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955421e1",
   "metadata": {},
   "source": [
    "## E5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cdd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e5_model = SentenceTransformer(\"intfloat/e5-large-v2\")\n",
    "\n",
    "def e5_text_embeddings(text: str) -> list[float]:\n",
    "    \"\"\"\n",
    "    Generate E5 embedding for a single text string.\n",
    "\n",
    "    The input will be prefixed with 'query:' as required for query encoding.\n",
    "    Output is a normalized vector (list of floats).\n",
    "    \"\"\"\n",
    "    text = str(text).strip()\n",
    "    if not text or not text.strip():\n",
    "        return [0.0] * e5_model.get_sentence_embedding_dimension()\n",
    "\n",
    "    text = f\"query: {text.strip()}\"\n",
    "    emb = e5_model.encode(text, normalize_embeddings=True)\n",
    "    return emb.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3f2d6",
   "metadata": {},
   "source": [
    "## Function for Padding Images for SigLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_to_square(img: Image.Image, fill_color=(255, 255, 255)) -> Image.Image:\n",
    "    w, h = img.size                             \n",
    "    size = max(w, h)                           \n",
    "    return ImageOps.pad(\n",
    "        img, \n",
    "        (size, size),                           \n",
    "        color=fill_color,                       \n",
    "        centering=(0.5, 0.5)                    \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a16a03",
   "metadata": {},
   "source": [
    "## SigLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bde683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_siglip_name = \"google/siglip-so400m-patch14-384\" \n",
    "_siglip_device = \"cpu\"\n",
    "_siglip_processor = SiglipProcessor.from_pretrained(_siglip_name)\n",
    "_siglip_model = SiglipModel.from_pretrained(_siglip_name).to(_siglip_device).eval()\n",
    "\n",
    "# Run a dummy forward pass once to infer the correct embedding dimension\n",
    "with torch.no_grad():\n",
    "    dummy = Image.new(\"RGB\", (384, 384), color=\"white\")\n",
    "    dummy_inputs = _siglip_processor(images=dummy, text=\"dummy\", return_tensors=\"pt\").to(_siglip_device)\n",
    "    dummy_output = _siglip_model(**dummy_inputs)\n",
    "    _siglip_dim = dummy_output.image_embeds.shape[-1]\n",
    "\n",
    "def siglip_image_embedding(image_path: str) -> list[float]:\n",
    "    \"\"\"\n",
    "    Compute a SigLIP image embedding from a local image path.\n",
    "    Returns an L2-normalized vector (list[float]).\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img = pad_to_square(img)\n",
    "        inputs = _siglip_processor(images=img, return_tensors=\"pt\").to(_siglip_device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vec = _siglip_model.get_image_features(**inputs)  \n",
    "            vec = F.normalize(vec, p=2, dim=-1)\n",
    "\n",
    "        return vec.squeeze(0).cpu().tolist()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return [0.0] * _siglip_dim\n",
    "\n",
    "def siglip_image_from_url(url: str) -> list[float]:\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        img = pad_to_square(img)\n",
    "        inputs = _siglip_processor(images=img, return_tensors=\"pt\").to(_siglip_device)\n",
    "        with torch.no_grad():\n",
    "            vec = _siglip_model.get_image_features(**inputs)  \n",
    "            vec = F.normalize(vec, p=2, dim=-1)\n",
    "        return vec.squeeze(0).cpu().tolist()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return [0.0] * _siglip_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edacaf8",
   "metadata": {},
   "source": [
    "## Qdrant Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41349eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qdrant = QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_URL\"),\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    "    \n",
    ")\n",
    "\n",
    "COLLECTION_NAME = \"products\"\n",
    "\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config={\n",
    "        \"image_cropped\": VectorParams(size=1152, distance=Distance.COSINE),\n",
    "        \"image_original\": VectorParams(size=1152, distance=Distance.COSINE),\n",
    "        \"text_gen_desc\": VectorParams(size=1024, distance=Distance.COSINE),\n",
    "        \"text_raw_desc\": VectorParams(size=1024, distance=Distance.COSINE),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e5427",
   "metadata": {},
   "source": [
    "## Inserting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5751343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"hedomak_products.csv\")\n",
    "\n",
    "\n",
    "points = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    point_id = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "    brand = row.get(\"brand\", \"\")\n",
    "    title = row.get(\"title\", \"\")\n",
    "    product_url = row.get(\"product_url\", \"\")\n",
    "    regular_price = row.get(\"regular_price\",\"\")\n",
    "    sale_price = row.get(\"sale_price\",\"\")\n",
    "    raw_desc = row.get(\"description\", \"\")\n",
    "    sizes = row.get(\"sizes\",\"\")\n",
    "    available = row.get(\"available\",\"\")\n",
    "    gen_desc = row.get(\"generated_desc\", \"\")\n",
    "    img_local = row.get(\"cropped_image_path\", \"\")\n",
    "    img_url = row.get(\"image_url\", \"\")\n",
    "\n",
    "    # Embeddings (functions handle failures internally)\n",
    "    vec_text_raw = e5_text_embeddings(raw_desc)\n",
    "    vec_text_gen = e5_text_embeddings(gen_desc)\n",
    "    vec_image_cropped = siglip_image_embedding(img_local)\n",
    "    vec_image_original = siglip_image_from_url(img_url)\n",
    "\n",
    "    # Sanitize payload (replace NaNs with None)\n",
    "    payload = {\n",
    "        \"brand\":brand,\n",
    "        \"title\": title,\n",
    "        \"product_url\":product_url,\n",
    "        \"regular_price\":regular_price,\n",
    "        \"sale_price\":sale_price,\n",
    "        \"description\":raw_desc,\n",
    "        \"sizes\":sizes,\n",
    "        \"available\":available,\n",
    "        \"image_url\": img_url,\n",
    "    }\n",
    "    payload = {k: (None if pd.isna(v) else v) for k, v in payload.items()}\n",
    "\n",
    "    # Build Qdrant point\n",
    "    point = PointStruct(\n",
    "        id=point_id,\n",
    "        vector={\n",
    "            \"text_raw_desc\": vec_text_raw,\n",
    "            \"text_gen_desc\": vec_text_gen,\n",
    "            \"image_cropped\": vec_image_cropped,\n",
    "            \"image_original\": vec_image_original,\n",
    "        },\n",
    "        payload=payload,\n",
    "    )\n",
    "\n",
    "    points.append(point)\n",
    "    \n",
    "\n",
    "print(f\"{len(points)} to upload...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26fed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 64\n",
    "\n",
    "for i in range(0, len(points), chunk_size):\n",
    "    try:\n",
    "        qdrant.upsert(\n",
    "            collection_name=\"products\",\n",
    "            points=points[i:i + chunk_size] \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error Upserting Batch {i // chunk_size + 1}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
