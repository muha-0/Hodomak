{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we crop the part of the image we need (based on product descriptiom) with Gdino and Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Function to clean filenames\n",
    "def clean_filename(title):\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '', title).replace(\" \", \"_\")\n",
    "# Load Grounding DINO Model\n",
    "model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device= \"cpu\" # Set to cpu for now\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Load CSV\n",
    "csv_file = \"hedomak_products.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Directory for cropped images\n",
    "output_dir = \"cropped_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load Zero-shot Classification Model \n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", framework=\"pt\", device = -1)\n",
    "\n",
    "contextualized_labels = {\n",
    "    \"topwear\": \"Clothing worn on the upper body, such as shirts, jackets, sweaters, hoodies.\",\n",
    "    \"bottomwear\": \"Clothing worn on the lower body, such as jeans, trousers, shorts, pants, leggings, denim.\",\n",
    "    \"full outfit\": \"A full set of clothing, including both topwear and bottomwear.\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Function to determine cropping category -> consider using Ai (BART)\n",
    "def get_cropping_label(description, title):\n",
    "        \n",
    "        # Labels for classification\n",
    "        labels = list(contextualized_labels.values())\n",
    "\n",
    "        result = classifier(title, labels)\n",
    "        predicted_value = result[\"labels\"][0]  # Get the highest confidence label\n",
    "        predicted_category = [key for key, value in contextualized_labels.items() if value == predicted_value][0]\n",
    "        score = result[\"scores\"][0]\n",
    "        if score > 0.85:\n",
    "            return predicted_category\n",
    "        \n",
    "        #Try the description\n",
    "        if pd.isna(description) or not isinstance(description, str) or description.strip() == \"\":\n",
    "            return \"full outfit\"\n",
    "        result = classifier(description, labels)\n",
    "        predicted_value = result[\"labels\"][0]  # Get the highest confidence label\n",
    "        predicted_category = [key for key, value in contextualized_labels.items() if value == predicted_value][0]\n",
    "        score = result[\"scores\"][0]\n",
    "        if score > 0.6:\n",
    "            return predicted_category\n",
    "        \n",
    "        \n",
    "        return \"full outfit\"\n",
    "\n",
    "# Function to process and crop image\n",
    "def process_image(image_url, product_title, description):\n",
    "    try:\n",
    "        # Load image\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            image = Image.open(response.raw).convert(\"RGB\")\n",
    "        else:\n",
    "            print(f\"Failed to download {image_url}\")\n",
    "            return None\n",
    "\n",
    "        original_image = image.copy()\n",
    "\n",
    "        cropping_label = get_cropping_label(description, product_title)\n",
    "        \n",
    "        def get_text_labels(cropping_label):\n",
    "            if cropping_label == \"topwear\":\n",
    "                return \"top wear, t-shirt, hoodie, jacket, sweater, shirt\"\n",
    "            elif cropping_label == \"bottomwear\":\n",
    "                return \"bottom wear, pants, jeans, trousers, shorts, leggings\"\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        # Determine cropping type\n",
    "        text_labels = get_text_labels(cropping_label) \n",
    "\n",
    "        if text_labels is None:\n",
    "            print(f\"Taking whole fit for {product_title}\")\n",
    "            original_image_path = os.path.join(output_dir, f\"{clean_filename(product_title)}.jpg\")\n",
    "            original_image.save(original_image_path)\n",
    "            return original_image_path\n",
    "\n",
    "        # Run model\n",
    "        inputs = processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Post-process results\n",
    "        results = processor.post_process_grounded_object_detection(\n",
    "            outputs,\n",
    "            inputs.input_ids,\n",
    "            box_threshold=0.2,\n",
    "            text_threshold=0.2,\n",
    "            target_sizes=[image.size[::-1]]\n",
    "        )\n",
    "\n",
    "        # If detection found\n",
    "        if len(results[0][\"boxes\"]) > 0:\n",
    "            max_index = results[0][\"scores\"].argmax()\n",
    "            box = [int(round(x)) for x in results[0][\"boxes\"][max_index].tolist()]\n",
    "\n",
    "            # Crop the detected object\n",
    "            cropped_image = original_image.crop((box[0], box[1], box[2], box[3]))\n",
    "\n",
    "            # Save cropped image\n",
    "            cropped_image_path = os.path.join(output_dir, f\"{clean_filename(product_title)}.jpg\")\n",
    "            cropped_image.save(cropped_image_path)\n",
    "\n",
    "            return cropped_image_path\n",
    "        else:\n",
    "            print(f\"No objects detected for {product_title}\")\n",
    "            original_image_path = os.path.join(output_dir, f\"{clean_filename(product_title)}.jpg\")\n",
    "            original_image.save(original_image_path)\n",
    "            return original_image_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {product_title}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process all products in the CSV\n",
    "for index, row in df.iterrows():\n",
    "    image_url = row[\"image_url\"]\n",
    "    product_title = row[\"title\"]\n",
    "    description = row[\"description\"]\n",
    "\n",
    "    new_image_path = process_image(image_url, product_title, description)\n",
    "    \n",
    "    if new_image_path:\n",
    "        df.at[index, \"cropped_image_path\"] = new_image_path  # Save new image path\n",
    "    \n",
    "\n",
    "\n",
    "df.to_csv(\"hedomak_products.csv\", index=False)\n",
    "\n",
    "print(\"Processing complete. Cropped images saved & CSV updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets generate detailed descriptions for these cropped images using LLava 1.6 13b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# ------------------------\n",
    "# CONFIG\n",
    "# ------------------------\n",
    "FILE_DIR = Path(r\"C:\\projects\\hedomak\\hedomak_products.csv\")\n",
    "df = pd.read_csv(FILE_DIR)\n",
    "\n",
    "#Remove\n",
    "df['generated_desc'] = \"\"\n",
    "df.to_csv(FILE_DIR, index=False)\n",
    "#######\n",
    "\n",
    "if 'generated_desc' not in df.columns:\n",
    "    df['generated_desc'] = \"\"\n",
    "\n",
    "# ------------------------\n",
    "# GENERATION FUNCTION\n",
    "# ------------------------\n",
    "def generate_description(image_path):\n",
    "    \"\"\"Read an image from local path and generate a detailed description using LLaVA.\"\"\"\n",
    "    try:\n",
    "        # Open the image and convert it to bytes\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        with BytesIO() as buffer:\n",
    "            img.save(buffer, format='PNG')\n",
    "            image_bytes = buffer.getvalue()\n",
    "\n",
    "        # Prompt for LLaVA\n",
    "        prompt = (\n",
    "        \"You are describing an apparel item for a fashion search index. \"\n",
    "        \"Return ONE LINE under 30 words, exactly in this order: \"\n",
    "        \"type; fit; length; material; primary color; texture/fabric feel; pattern/print; sleeve type; neckline/collar; rise/waistline; closure type; notable details; seasonality; style/occasion tags.\\n\"\n",
    "        \"If you are not sure about any attribute, write 'unknown' instead of leaving it blank. \"\n",
    "        \"Do not add extra commentary.\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"jeans; relaxed; full-length; rigid denim; black; matte; solid; no sleeves; n/a; high-rise; button fly; clean hem; all-season; casual,minimal\\n\"\n",
    "        \"hoodie; oversized; hip-length; cotton blend; heather grey; soft fleece; solid; long sleeves; hood; n/a; pullover; kangaroo pocket; winter; streetwear,casual\\n\"\n",
    "        \"Now describe the item in the image:\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Generate caption\n",
    "        full_response = ''\n",
    "        for response in ollama.generate(\n",
    "            model='llava:7b-v1.6',\n",
    "            prompt=prompt,\n",
    "            images=[image_bytes],\n",
    "            stream=True,\n",
    "            options = {\n",
    "            \"num_gpu\": 0,          # CPU-only if you need it\n",
    "            \"temperature\": 0.2,    # concise & consistent\n",
    "            \"top_p\": 0.9,          # a touch of variety without drift\n",
    "            \"repeat_penalty\": 1.05,# reduces duplicate phrases\n",
    "            \"num_predict\": 80,     # single-line cap\n",
    "            \"stop\": [\"\\n\"],        # cut at first newline (enforces one line)\n",
    "            \"keep_alive\": \"10m\",   # keep model warm between calls\n",
    "            \n",
    "            }\n",
    "        ):\n",
    "            full_response += response['response']\n",
    "\n",
    "        return full_response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ------------------------\n",
    "# LOOP OVER FOLDER\n",
    "# ------------------------\n",
    "\n",
    "#desc = generate_description(img_path)\n",
    "x = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.isna(row['generated_desc']) or str(row['generated_desc']).strip() == \"\":\n",
    "        desc = generate_description(df.at[idx, 'cropped_image_path'])\n",
    "        print(f\"{desc} \\n{'-'*50}\")\n",
    "        df.at[idx, 'generated_desc'] = desc\n",
    "        x+=1\n",
    "        if x%5==0:\n",
    "            df.to_csv(FILE_DIR, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
